---
title: "Data Wrangling"
subtitle: "STAT 231 Blog Project"
author: "Gloria Wu, Justyce Williams, Ben Snyderman"
date: today
format: pdf
linestretch: 1.15
highlight-style: arrow
---

```{r}
#| label: setup
#| include: FALSE

knitr::opts_chunk$set(
  # display code as types
  tidy = FALSE, 
  # slightly smaller code font
  size = "small",
  # do not display messages in PDF
  message = FALSE,
  # set default figure width and height
  fig.width = 5, fig.height = 3) 

# improve digit and NA display 
options(scipen = 1, knitr.kable.NA = '')

# load packages
library(tidyverse)
library(rvest)
library(ggplot2)
library(stringr)
library(purrr)
library(robotstxt)
library(tidytext)
library(tidyr)
library(dplyr)
library(spotifyr)
library(tidytuesdayR)
library(geniusr)
library(httr)
library(jsonlite)
library(broom)
library(GGally)
library(rgenius)


```

## Variables


## Wrangling

```{r}
#| label: load & basic wrangling

## check if we can scrape

# genius
paths_allowed("https://genius.com")

# spotify
paths_allowed("https://spotify.com")

# Individual Song Lyrics Website Allows us to Scrape
paths_allowed("https://open.spotify.com/track/00Cwaioho8d1pETtPFO79T")

# Spotify DataSet 
spotify_songs <- readRDS("raw-data/tt_spotify_songs.Rds")

#Wikipedia 
paths_allowed("https://en.wikipedia.org/wiki/Post_Malone") # Post Malone 
# Just to check if Wikipedia lets us scrape their website 










# All Songs - Full Set for Cluster 
# get rid of duplicate songs
spotify_distinct <- spotify_songs|>
   distinct(track_name, .keep_all = TRUE) #For dataframes only

# Popular Songs
songs_top100 <- spotify_distinct |>
  filter(track_popularity >= 87) |>
  select(- track_album_id, - track_album_release_date,
         - playlist_name, -playlist_id, -playlist_id) |>
  group_by(track_id)
```

```{r}
#| label: scrape lyrics from genius.com
#Genius Lyric Data

Sys.getenv("ADf5awyFcKidqW0da12mAzZsO8U_PQ6XDrF3ecWrhbqJzrQM1-lM9fysJFWk_K79")

# Will need to figure out how to make this a full data frame eventually 

# Helpful Website: https://ewenme.github.io/geniusr/

#lyrics
lyrics <- get_lyrics_search(artist_name = "Kanye West",
                  song_title = "Good Morning") 
```

```{r}
#| label: (attempt) load spotify data - DELETE IF NOT USED

## spotify credentials
##Sys.setenv(SPOTIFY_CLIENT_ID = "03faef051fef46ed8ecc6f2153a55206")
##Sys.setenv(SPOTIFY_CLIENT_SECRET = "475255efd20c45f88e50275490895841")

## tokens necessary for use 
#access_token <- get_spotify_access_token()

## get playlist details
#playlist <- get_playlist_tracks("6UeSakyzhiEt4NB3UAd6NQ")

## get individual track audio features
# track_ids <- playlist$track.id
# track_audio_features <- get_track_audio_features(track_ids,
#                                                  authorization = access_token)
```


```{r}
#| label: scrape genius lyrics 

## chatGPT prompt: scrape lyrics from genius.com

genius_access_token <- "W28ITauMuo8HwVUStqe7xzv0EepDLZm3rHvYkkAHyhrp-0Y-FSdm7GDqFqCYS-fA"

## pre-allocate space
playlist_lyrics <- tibble(track_name = rep(NA_character_, 100),
                          track_id = rep(NA_real_, 100),
                          lyrics = rep(NA_character_, 100))

## iterate through to get lyrics
for (i in 2:100){
  # scrape lyrics
  song_title <- playlist$track.name[i]
  search_song <- GET("https://api.genius.com/search",
                     query = list(q = song_title),
                     add_headers(Authorization = paste("Bearer", genius_access_token)))
  song_info <- content(search_song, as = "parsed", type = "application/json")
  song_url <- song_info$response$hits[[1]]$result$url
  lyrics <- read_html(song_url) |>
    html_element("div[data-lyrics-container='true']") |>
    html_text2()
  
  # add to dataset
  playlist_lyrics$track_name[i] <- song_title
  playlist_lyrics$track_id[i] <- playlist$track.id[i]
  playlist_lyrics$lyrics[i] <- lyrics
}

## need to manually clean
## need to manually scrape:
  # 1. luther
```

```{r}
#| label: wrangling for k-means analysis
# remove any non-numerical variables
spotify_kmeans <- spotify_distinct |>
  select(- track_id, - track_name, - track_artist, - track_album_id,
         - track_album_name, - track_album_release_date, - playlist_name,
         - playlist_id, -playlist_genre, - playlist_subgenre)

# standardize variables
spotify_kmeans_standardized <- spotify_kmeans |>
  mutate(across(where(is.numeric),
                ~ (.x - mean(.x)) / sd(.x),
                .names = "{.col}_z")) |>
  select(ends_with("_z"))

# elbow plot to see optimal amount of clusters
elbow_plot <- tibble(k = 1:10) |>
  mutate(
    kmeans_results = purrr::map(k, ~kmeans(spotify_kmeans_standardized,
                                           .x, nstart = 20)),
    glanced = purrr::map(kmeans_results, glance)) |>
  unnest(cols = c(glanced))

elbow_plot |>
  ggplot(aes(x = k, y = tot.withinss)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Number of clusters (k)",
       y = "Total within-cluster sum of squares")

# k-means analysis (4 clusters)
set.seed(777)

spotify_kmeans_4 <- spotify_kmeans_standardized |>
  kmeans(centers = 4, nstart = 20)

spotify_k3 <- augment(spotify_kmeans_4, spotify_kmeans_standardized)
```

```{r}
#| label: wrangling for Sentiment Analysis


##Still working on removing filler at the top
playlist_lyrics <- tibble(
  track_name = rep(NA_character_, 91),
  track_id = rep(NA_character_, 91),
  lyrics = rep(NA_character_, 91)
)



for (i in 1:91) {
  song_title <- songs_top100$track_name[i]
  song_id <- songs_top100$track_id[i]
  
  search_song <- GET("https://api.genius.com/search",
                     query = list(q = song_title),
                     add_headers(Authorization = paste("Bearer", "OQQUZXvKYhXxNvw9_x7RZgVypJWWwUQvSGmJMD77mTonSK75psaEQL48qYbIpfaj")))
  song_info <- content(search_song, as = "parsed", type = "application/json")


    song_url <- song_info$response$hits[[1]]$result$url

    lyrics <- 
      read_html(song_url) |>
        html_elements("div[data-lyrics-container='true']") |>
        html_text2() |>
        paste(collapse = "\n")
    


    playlist_lyrics$track_name[i] <- song_title
    playlist_lyrics$track_id[i] <- song_id
    playlist_lyrics$lyrics[i] <- lyrics
  
}

##used ChatGPT prompt to help get (?s)^.*?\\bLyrics\\b\\s*\\n+ within the str_replace method
playlist_lyrics_2 <- playlist_lyrics |>
  mutate(lyrics = str_replace(lyrics, "(?s)^.*?\\bLyrics\\b\\s*\\n+", ""))

playlist_lyrics_3 <- playlist_lyrics_2 |>
  mutate(lyrics = str_replace(lyrics, "(?s)^.*?\\bRead More\\b\\s*\\n+", ""))

playlist_lyrics_4 <- playlist_lyrics_3 |>
  mutate(lyrics = str_remove_all(lyrics, "\\[.*?\\]"))

lyrics_words <- playlist_lyrics_4 |>
  unnest_tokens(output = word, input = lyrics)

afinn_lexicon <- get_sentiments("afinn")


afinn_lyrics <- lyrics_words |>
  inner_join(afinn_lexicon, by = "word")



afinn_song_scores <- afinn_lyrics |>
  group_by(track_name, track_id) |>
  summarize(sentiment_score = sum(value))


my_stop_words <- tibble(
  word = c("contributors", "Translations")
)





lyrics_no_stop_words <- lyrics_words |>
  anti_join(stop_words, by = "word") |>
  anti_join(my_stop_words, by = "word")


word_counts <- lyrics_no_stop_words |>
  count(word)


songs_top100_genre <- songs_top100 |>
  select(track_id, playlist_genre)

playlist_lyrics_4 <- playlist_lyrics_4 |>
  left_join(songs_top100_genre, by = "track_id")
         
```

```{r}
#| label: Wrangle for Network 

#Edit Names For Wikipedia Urls 
#Some artists names are too vague, forcing R to quit reading through because it can't find the specfic urls
songs_top100<- songs_top100|>
  mutate(track_artist= case_when(
    track_artist == "MEDUZA" ~ "MEDUZA (producers)",
    TRUE ~ track_artist
  ))

#Urls Iteration for Each Artisit in Comprehensive List
songs_top100_urls<- songs_top100 |>
  mutate( 
    urlartist=  gsub(" ", "_", track_artist), #creates a space for the url to go first, and the separation of the artist names with a underscoe instead of a space because that is how Wikipedia works 
    url= paste0("https://en.wikipedia.org/wiki/", urlartist) #creates the base website URL 
  )


#Save and Read Urls
songs_top100_urls_html <- songs_top100_urls$url[1] |>
  unique() |>
  read_html()|>


    # Also works with "div.wst-plainlist > ul > li"
    html_elements("a") |>
    html_text2()

as.data.frame(unique(songs_top100_urls_html)) 



```


```{r}


vec <- pull(songs_top100_urls, url)
for(v in vec) {
  cat(v)
  read_html(v)
}









songs_top100_urls <- songs_top100_urls |>
  mutate( texthtml = map(url, (read_html))
  )



#Reading the Website w/ HTML
wikihtml <- songs_top100_urls$url|>
  map(songs_top100_urls$url, read_html)
#Current reading in all the values at Null 


 # Identify number of iterations (start with 1!)
n_iter <- nrow(songs_top100_urls)
  # Pre-allocate space
  songs_wikipedia <- songs_top100_urls|>
    tibble(title = titles
                           , url = urls
                            # Create empty character vector the same
                           # length as our titles vector
                           , text = vector(mode = "character"
                                           , length = length(titles))) |>
# Limit to only the rows we are trying to work with
  slice(1:n_iter)
# Iterate through links 
  for (i in 1:n_iter) {
    songs_wikipedia$text[i] <- songs_wikipedia$url[i] |>
      read_html() |>
      html_element("#mw-content-text") |>
      html_text2()
}
  # Identify number of iterations (start with 1!)
  n_iter <- nrow(toc)
  # Pre-allocate space
  songs_wikipedia <- tibble(title = titles
                           , url = urls
                           # Create empty character vector the same
                           # length as our titles vector
                           , text = vector(mode = "character"
                                           , length = length(titles)))


#Pulling by Paragraphs
paragraphs <-wikihtml|>
  map(~ .x |> #for each wikipedia page url 
      html_nodes("p")|> 
      html_text() |> 
      paste(collapse = " "))
  

#Fromn Lab to help make sense 
toc_url |>
  read_html() |> 
  html_elements("div.wst-plainlist> ul>li>a") |>
  html_attr("href")



```





```{r}
#| label: save final objects


```

## Sources
